# Databricks Certified Generative AI Engineer Associate â€“ Study Notes

## ğŸ§± Section 1: Design Applications
*How to architect GenAI solutions from use-case to execution flow*

---

### 1ï¸âƒ£ Prompt Design Rules

- **For structured output** â†’ Use few-shot prompting with clearly labeled examples.  
  Example:  
  `"Respond in JSON with keys â€˜summaryâ€™, â€˜sentimentâ€™, and â€˜entitiesâ€™."`

- **For classification tasks** â†’ Use zero-shot prompts with explicit classes:  
  `"Classify as one of: positive, neutral, negative."`

- **To elicit formatted output** â†’ Include formatting instructions **after** core context.  
  Models prioritize recent tokens â†’ put format instructions last.

- **Use system prompts** to establish tone/persona:  
  `"You are a financial assistant who responds concisely in bullet points."`

---

### 2ï¸âƒ£ Task-to-Model Mapping

| Business Need                          | Model Task                  | Preferred Approach                        |
|---------------------------------------|-----------------------------|-------------------------------------------|
| Text summarization                    | Summarization               | Prompt LLM with extractive/abstractive instructions |
| Structured data extraction (PDF/HTML) | Information Extraction      | Retrieval + field-specific prompt         |
| Classification                        | Zero-/Few-shot Classification | Prompt with example-labeled outputs     |
| Q&A over documents                    | Retrieval-Augmented Generation (RAG) | Embed + retrieve + prompt       |
| Multi-step reasoning or action        | Agent                        | Tool selection + LLM + memory             |

> ğŸ”‘ **Default**: Use **retrieval + prompting** before considering fine-tuning.

---

### 3ï¸âƒ£ Choosing Chain Components

**Input-based decisions:**
- Raw text â†’ Embed + index in Vector DB
- PDF/HTML â†’ Chunk + tag with metadata
- SQL or structured input â†’ Function calling

**Output-based decisions:**
- Summary â†’ Summarization chain
- Grounded answer â†’ RAG (Retriever + Generator)
- Action (e.g. booking) â†’ Agent with tools

---

### 4ï¸âƒ£ Translating Business Goals into Pipeline Design

Use this mental checklist:

- âœ… What is the **goal**?  
- âœ… What **input format** is expected?  
- âœ… What **output format** is required?  
- âœ… Is **factual accuracy** required?

**Example:**

> *"Generate ticket replies based on internal KB."*

- Input: ticket text  
- Output: concise email response  
- Pipeline:  
  - Chunk & embed KB  
  - Vector search relevant chunks  
  - Prompt LLM with user ticket + top docs  
  - Format output in helpdesk response style

---

### 5ï¸âƒ£ Multi-Stage Reasoning and Tool Use

- **Tools** = APIs or functions (e.g. calculator, search, lookup)
- **Agents** = LLMs that plan tool use across steps
- **Chain-of-thought prompting** â†’ Break complex reasoning into steps

**Example Tool Chain:**

1. Extract keywords from user question  
2. Use keywords to search KB  
3. Summarize retrieved docs  
4. Generate structured answer or invoke function

> ğŸ” RAG = static retrieval â†’ generate.  
> ğŸ§  Agents = dynamic reasoning â†’ tool â†’ loop.

---

### ğŸ¯ Key Rules of Thumb

- Use RAG before fine-tuning â†’ cheaper, safer, faster.
- Use system prompts to control tone and intent.
- Favor few-shot examples for structured output.
- Chain components based on **input/output**, not tool familiarity.
- Multi-stage reasoning? Consider **agents + tools**.

---

## ğŸ§¾ Section 2: Data Preparation
*How to structure, clean, chunk, and persist content for retrieval-augmented generation (RAG) pipelines*

---

### 1ï¸âƒ£ Chunking Strategies

- **Default to semantic chunking** â†’ Split on paragraph/topic boundaries, not fixed size.
  - Better semantic integrity â†’ improves relevance of retrieved chunks.
  - Avoid mid-sentence or mid-thought cuts â†’ lowers grounding quality.

- **Use overlapping sliding windows** for dense information (e.g. scientific docs).
  - Ensures context continuity even when chunks are small.

- **Adjust chunk size to model context window**:
  - Small model (4k tokens) â†’ 256â€“512 token chunks
  - Larger model (32k) â†’ 1kâ€“2k token chunks

- **Chunk metadata**:
  - Always attach `source`, `page_number`, `section_title`, etc. for filtering and reranking.

---

### 2ï¸âƒ£ Filtering and Cleaning Documents

- **Remove noise before chunking**:
  - Ads, headers, footers, table of contents, legal disclaimers, repeated boilerplate.

- **Normalize input text**:
  - Fix broken line breaks, remove HTML tags, decode special characters.

- **Avoid over-including irrelevant sections**:
  - E.g., donâ€™t embed entire manuals or books â€” focus on FAQs, high-quality docs, curated knowledge bases.

- **Prioritize quality over quantity**:
  - More chunks â‰  better retrieval â€” irrelevant docs pollute results.

---

### 3ï¸âƒ£ Python Packages for Document Parsing

| Format        | Recommended Package           |
|---------------|-------------------------------|
| PDF           | `PyMuPDF` (fitz), `pdfplumber` |
| Word (DOCX)   | `python-docx`                 |
| HTML          | `BeautifulSoup`               |
| Markdown      | `markdown` + `html2text`      |
| Scanned PDFs  | `pytesseract` (OCR required)  |

> ğŸ”‘ Use parsers that preserve layout and sectioning where possible.

---

### 4ï¸âƒ£ Writing Chunks to Delta Lake in Unity Catalog

- Store output as **structured tables** for governance and traceability:
  - Columns: `chunk_text`, `embedding_vector`, `document_id`, `metadata`

- Sequence:
  1. Extract â†’ Clean â†’ Chunk â†’ Embed
  2. Write to Delta table (`spark.write.format("delta")`)
  3. Register table in Unity Catalog (`CREATE TABLE ... USING DELTA`)

- Use `AUTOINCREMENT` or UUIDs for chunk ID column
- Maintain lineage by tracking source file paths and timestamps

> Unity Catalog ensures access control and metadata lineage for chunked data.

---

### 5ï¸âƒ£ Source Document Selection for RAG

- Use **authoritative**, **concise**, and **updated** documents
  - Avoid:
    - Raw logs
    - Social media
    - Long unstructured books or legal docs without filtering

- Favor:
  - Internal knowledge bases
  - FAQs
  - Product documentation
  - Wikis with change control

> Choose sources that align with use-case scope and tone.

---

### 6ï¸âƒ£ Prompt/Response Pair Selection

- Match prompt types to model capability:
  - Classification â†’ Short input, clear output labels
  - Summarization â†’ Long input, short output
  - Retrieval QA â†’ Grounded context + question â†’ answer

- Filter out:
  - Noisy or inconsistent labels
  - Overly long or ambiguous prompts
  - Hallucinated responses

- Use these pairs for:
  - Evaluation
  - Fine-tuning (if needed)
  - Retrieval-based simulation

---

### 7ï¸âƒ£ Retrieval System Evaluation & Re-Ranking

- **Use metrics to evaluate retrieval**:
  - Precision@k â†’ % of relevant chunks in top k
  - Recall@k â†’ % of total relevant chunks retrieved
  - NDCG â†’ Rewards good ranking order
  - MRR (Mean Reciprocal Rank) â†’ Focus on getting top-1 right

- **Re-ranking boosts quality**:
  - First stage: vector similarity (fast, dense)
  - Second stage: reranker (cross-encoder LLM or heuristic scoring)

> Re-rankers compare **question + passage together** and rescore.

---

### ğŸ¯ Key Rules of Thumb

- Use semantic chunking with overlaps for best retrieval quality.
- Clean and normalize before embedding â€” garbage in = garbage out.
- Store chunks in Delta Lake with rich metadata â†’ enables smart filtering + governance.
- Retrieval systems work best with high-quality, targeted source docs.
- Evaluate your retriever using precision/recall and apply reranking if quality is low.

---

## ğŸ› ï¸ Section 3: Application Development
*Building, evaluating, and securing GenAI apps using prompts, agents, tools, and frameworks*

---

### 1ï¸âƒ£ Retrieval-Aware Development

- **Create tools to extract data**:
  - Write wrapper functions for search, lookup, or internal APIs
  - Integrate data extraction tools (e.g., OCR, SQL, REST APIs) into RAG pipelines

- **Select chunking strategy based on evaluation**:
  - Poor recall â†’ try smaller chunks or increase overlap
  - Irrelevant results â†’ better metadata filtering or re-ranking

- **Embed with context length in mind**:
  - Long source docs â†’ split into manageable chunks
  - Align chunk size with embedding modelâ€™s max token size

---

### 2ï¸âƒ£ Prompt Engineering in Practice

- **Augment prompts with context**:
  - Dynamically insert fields like `customer_name`, `product_type`, or recent actions
  - Use string templating: `f"Given that the user purchased {product_type}, recommend..."`

- **Adjust output via prompt rewrites**:
  - Original: â€œSummarize the following.â€  
    Improved: â€œSummarize the following in 2 bullet points for a 5th grader.â€

- **Use system prompts for baseline behavior**:
  - `"You are a friendly assistant."` or `"Always respond concisely and include source links."`

- **Write metaprompts to reduce hallucination**:
  - `"Only respond using information from the provided context. If unsure, say 'I donâ€™t know'."`

---

### 3ï¸âƒ£ Model & Embedding Selection

- **Select LLMs based on attributes**:
  - Open-weight: Llama 2, MPT â†’ good for cost control, internal deployment
  - Proprietary: GPT-4, Claude â†’ strong reasoning, broader context window
  - MosaicML â†’ for custom fine-tuning, hosted models

- **Key model metadata to check**:
  - Context window size  
  - Training dataset transparency  
  - Eval metrics (MMLU, HELM, TruthfulQA)

- **Use cases by type**:
  - Text classification â†’ smaller model or distilled variant
  - Chat/agentic behavior â†’ instruction-tuned, high-context model
  - Tool use / multi-function â†’ model with ReAct or function-calling support

- **Choose embedding model** based on:
  - Query complexity and domain
  - Document length
  - Cost/performance tradeoff

---

### 4ï¸âƒ£ Frameworks and Chains

- **LangChain or similar tools**:
  - Use for chaining steps: retrieval â†’ transform â†’ generation
  - Supports agents, tools, chat memory, and retrievers

- **Agent Frameworks (LangChain, Semantic Kernel, etc.)**:
  - Enable tool selection at runtime
  - Useful for multi-step tasks: search â†’ analyze â†’ generate â†’ act

- **Agent prompt template** example:

## ğŸš€ Section 4: Assembling and Deploying Applications
*How to build, register, serve, and secure GenAI applications on the Databricks platform*

---

### 1ï¸âƒ£ Building and Coding Chains

- **LangChain chains** (common exam focus):
  - Chain = pipeline of components: retriever â†’ prompt â†’ LLM â†’ output parser
  - `SimpleSequentialChain`, `LLMChain`, and `RetrievalQA` are key constructs

- **Pyfunc model chaining (MLflow)**:
  - Wrap pre-processing â†’ LLM call â†’ post-processing as a single `pyfunc` model
  - Register using `mlflow.pyfunc.log_model()` with a custom `PythonModel` class

- **Example use case**:
  - Pre: clean input, lookup metadata  
  - Core: invoke LLM or retriever  
  - Post: validate format, redact PII

> ğŸ”§ Know how to encode a basic retrievalâ†’generate flow either in LangChain or custom pyfunc model.

---

### 2ï¸âƒ£ Registering and Serving Models

- **Register models in Unity Catalog**:
  - Use `mlflow.register_model()` or via UI
  - Model versioning is automatic
  - Attach signature (`input_schema`, `output_schema`) to ensure compatibility

- **Deployment steps (for RAG app)**:
  1. Ingest and chunk source data
  2. Generate embeddings (e.g., HuggingFace, OpenAI)
  3. Create and persist a vector index
  4. Wrap prompt + retriever into a LangChain or pyfunc chain
  5. Log the model with MLflow
  6. Register model in Unity Catalog
  7. Deploy to Model Serving

- **Control access** using Unity Catalog permissions or API gateway tokens

---

### 3ï¸âƒ£ Serving Vector Search and LLM APIs

- **Databricks Vector Search**:
  - Fully managed vector index service (dense and hybrid search supported)
  - Backed by Delta tables
  - Use `CREATE VECTOR INDEX` SQL or Python SDK
  - Query via `.query()` or SQL `ai_query()`

- **Vector Index Components**:
  - Source Delta Table with embedded column
  - Metadata for filtering (e.g., tags, doc_id, user_role)
  - Index type (dense or hybrid)

- **Mosaic AI Vector Search**:
  - Similar to Databricks Vector Search
  - Focused on large-scale retrieval + tight LLM integration
  - May appear on exam in name only (know it exists and does what Vector Search does, but scaled)

- **ai_query() for batch inference**:
  - SQL function to embed and search in batch (ideal for offline scoring)
  - `SELECT ai_query('vector_index_name', question_column) AS answer`

---

### 4ï¸âƒ£ Foundation Model Serving

- **Serving hosted models via Databricks**:
  - GPT-4, Claude, LLaMA 2 via MosaicML or third-party APIs
  - Use `dbx.ChatCompletion.create()` or compatible SDKs
  - Use Databricks Model Serving (managed or serverless) for endpoint hosting

- **Resources required to serve RAG app**:
  - Model artifacts (LLM + retriever)
  - Vector search index
  - Serving cluster (serverless or managed)
  - Unity Catalog for access control
  - Prompt templates or chain config (JSON, LangChain YAML, etc.)

> ğŸ§  Exam focuses on sequencing these components correctly for deployment.

---

### ğŸ¯ Key Rules of Thumb

- Prefer **LangChain for chaining** unless pyfunc custom logic is required.
- Register all production models in **Unity Catalog** for governance.
- Use **Vector Search + ai_query()** for low-latency embedding-based lookups.
- Sequence = ingest â†’ chunk â†’ embed â†’ index â†’ chain â†’ model registry â†’ endpoint
- Secure model serving with **ACLs or token-based API access**.
- For batch workloads â†’ use `ai_query()` inside SQL or scheduled notebooks.

---

## ğŸ›¡ï¸ Section 5: Governance
*Enforcing safety, compliance, and content integrity in Generative AI applications*

---

### 1ï¸âƒ£ Masking Techniques for Guardrails

- **Why use masking**:
  - Protect sensitive data (e.g., PII) from leaking into prompts or model outputs.
  - Prevent performance degradation from noisy/unstructured inputs.

- **Common masking patterns**:
  - Regex-based scrubbing (e.g., emails, phone numbers)
  - Named Entity Recognition (NER) â†’ auto-detect entities to mask
  - Pre-processing step before chunking or embedding

- **Masking use cases on exam**:
  - Improve retrieval quality by removing user-specific tokens.
  - Prevent prompts from overfitting to irrelevant patterns.

> ğŸ§  Example: Mask `user_id` before storing in embedding index â†’ improves generalization + avoids leakage.

---

### 2ï¸âƒ£ Guardrails Against Malicious Input

- **Input filtering**:
  - Reject or flag queries with prompt injection patterns.
  - Block known jailbreak phrases (e.g., â€œignore the previous instructionâ€).

- **System-level prompts**:
  - `"You are an assistant that only responds with factual, safe content."`

- **Rate limiting / abuse detection**:
  - Apply usage quotas or anomaly detection on user inputs

- **Output validation**:
  - Use regex or classifiers to reject dangerous content (e.g., hate speech, threats)

> ğŸ” Combine **input filtering**, **system prompts**, and **output sanitization** for full coverage.

---

### 3ï¸âƒ£ Problematic Text Mitigation in Source Data

- **Common issues**:
  - Toxic language, hallucinated data, misinformation, PII, offensive terms

- **Mitigation strategies**:
  - Text classification: detect & flag at ingestion
  - Data redaction: remove or mask terms before chunking
  - Embedding filtering: exclude high-risk chunks from index

> âœ… Prefer *removal or replacement at ingest time*, not after serving.

---

### 4ï¸âƒ£ Licensing and Legal Considerations

- **Know your source**:
  - Donâ€™t embed or serve outputs from copyrighted content without permission
  - Prefer open license datasets (e.g., Creative Commons, Apache 2.0)

- **Regulatory awareness**:
  - GDPR: avoid storing or exposing user PII without legal basis
  - Copyright: donâ€™t fine-tune or RAG on content under closed license

- **Recommended practices**:
  - Maintain metadata on source licenses
  - Use attribution if required (via prompt injection or citations)

> ğŸ“š If in doubt, donâ€™t index it â€” ensure data is legally safe *before* embedding or serving.

---

### ğŸ¯ Key Rules of Thumb

- Mask sensitive info early â†’ improves performance and security.
- Use input/output guardrails to block prompt injection and toxic responses.
- Pre-filter problematic documents â€” donâ€™t assume the LLM will "handle it."
- Always verify data source licenses â†’ avoid legal exposure in RAG chains.
- Use Unity Catalog to track data lineage and enforce access policies where applicable.

---

## ğŸ“ˆ Section 6: Evaluation and Monitoring  
*Measuring, improving, and maintaining LLM performance in production RAG systems*

---

### 1ï¸âƒ£ LLM Selection Based on Metrics

- **Quantitative metrics to compare models**:
  | Metric           | Use Case                                      |
  |------------------|-----------------------------------------------|
  | MMLU             | Academic/general knowledge benchmarking       |
  | TruthfulQA       | Hallucination resistance                      |
  | GSM8K            | Math and reasoning                            |
  | ARC              | Multi-step question answering                 |
  | HELM             | Overall evaluation framework (accuracy + bias)|

- **Rule of thumb**:  
  - Choose **smaller models** (e.g., MPT, LLaMA 7B) when:
    - Cost is key, and latency is critical  
    - Task is simple or repetitive
  - Choose **larger models** (e.g., GPT-4, Claude 2) when:
    - High accuracy or nuance is required  
    - Multi-step reasoning is needed

---

### 2ï¸âƒ£ Key Monitoring Metrics

| Scenario                        | Metric to Monitor                        |
|--------------------------------|------------------------------------------|
| General LLM health             | Token usage, latency, error rate         |
| RAG performance                | Retrieval hit rate, context overlap, hallucination %
| Chatbot/agent quality          | User thumbs up/down, task success rate   |
| Compliance/safety              | Blocked output %, PII leakage detections |

- Use **custom metrics** in MLflow or inference tables for:
  - Prompt length, response length
  - Response time buckets
  - API error codes

---

### 3ï¸âƒ£ Using MLflow for Evaluation

- Track:
  - Prompt templates
  - Retrieved context
  - Model outputs
  - Eval scores

- Log with:
  ```python
  mlflow.log_metrics({'bleu': 0.89, 'rouge': 0.76})
  mlflow.log_params({'model_name': 'llama2', 'chunk_size': 512})

## ğŸ§  Additional Study Notes

---

### ğŸ” RAG Pipelines & Optimization

- **End-to-End Flow**: Ingest â†’ preprocess â†’ chunk â†’ embed â†’ index â†’ retrieve â†’ prompt â†’ generate.
- **Chunking & Evaluation**:
  - Choose chunk strategy (semantic, sliding window) based on Recall@k or NDCG.
  - Use `LLM-as-a-judge` or human evals to tune retrieval performance.
- **Indexing**: Use Delta tables + Databricks Vector Search to manage embeddings.
- **Retrieval Tip**: Use metadata filters (e.g. `book="1"`) to narrow scope in dense indexes.

---

### ğŸ§  Databricks Vector Search & Index Management

- **Databricks Vector Search**:
  - Indexes Delta tables with embedded chunks.
  - Query with `.search()` or SQL `ai_query()`.
- **Supports hybrid search**: keyword + embedding for better relevance.
- **Similarity Metric**: Default is L2 via HNSW. Normalize embeddings for cosine similarity.
- **Filtering**: Add `section`, `doc_type`, or `user_role` as metadata filters.

---

### âš¡ Real-Time Data with Feature Store

- Use **Databricks Feature Store + Feature Serving** to supply live, structured inputs.
- Example: Serve up-to-date delivery times, account balances, or session state.
- Complements RAG by grounding with real-time facts.
- Secure via Unity Catalog + auto-scaling endpoints.

---

### ğŸš€ Model Serving & AI SQL Functions

- **Model Deployment Options**:
  - MLflow PyFunc â†’ wrap chains or preprocessors.
  - Hosted foundation models via MosaicML â†’ use `dbx.ChatCompletion()`.
- **AI Functions**:
  - SQL-native: `ai_query()` or `llm_query()` for inline LLM inference.
  - Useful for dashboards and reporting.

---

### ğŸ§° MLflow PyFunc + Secrets Best Practices

- Use **PyFunc** for chaining LLM calls with custom logic.
- Avoid `spark.conf.set()` for secrets â€” use environment variables or Databricks Secrets.
- Track prompt templates, metrics, input/output examples in MLflow runs.

---

### ğŸ” Unity Catalog Governance

- Store all data (chunks, features, logs) in Unity Catalog-backed tables.
- Use UC to manage ACLs, model promotion, and audit trails.
- Enable table and model lineage for traceability.

---

### ğŸ¤– Agents, Tools, & Multi-Step Reasoning

- **Agent Use Case**: When an LLM needs to decide dynamically between tools (e.g., RAG vs SQL vs API call).
- Tool list is passed as part of system prompt.
- Use LangChain or Databricks Agents to build agents with memory and tool use.

---

### âœï¸ Prompt Engineering Enhancements

- **Input Augmentation**: Dynamically insert user-specific context (e.g. `plan_type`, `locale`, `account_status`).
- **Output Formatting**: Guide model with structure (e.g. `"Respond in JSON with fields X, Y, Z"`).
- **Pre-processing**: Use PyFunc to sanitize inputs, enforce format, or append extra context before prompt hits LLM.

---

### ğŸ“Š Model Selection & Trade-offs

| Model Type            | Use Case                                      |
|-----------------------|-----------------------------------------------|
| Small Open Models     | Low-latency tasks (e.g. classification, tagging) |
| Large Open Models     | Reasoning-heavy RAG with self-hosted options  |
| Proprietary Models    | Complex summarization, conversational agents  |

- Check context length, MMLU scores, architecture type when comparing.
- Use proprietary APIs only if compliance risk is acceptable.

---

### ğŸ§ª Monitoring & Evaluation Metrics

- **Before deployment (eval)**:
  - BLEU/ROUGE for summarization
  - MMLU for general model strength
  - Retrieval: Precision@k, Recall@k, NDCG

- **Post-deployment (monitoring)**:
  - Token count, latency, success/failure rate
  - Retrieval relevance score
  - Output rejection rate (e.g. via safety guardrails)

---

### âš–ï¸ Guardrails: Types & Use Cases

| Type         | Purpose                                            |
|--------------|----------------------------------------------------|
| Safety       | Block offensive/harmful inputs                     |
| Compliance   | Enforce business or legal constraints (e.g. no politics) |
| Contextual   | Ensure outputs align with user scope/intended use |
| Evaluation   | Human or model-in-the-loop scoring & audits       |

- Use **input filters + output format checks + system prompt constraints**.
- Guardrails can wrap the entire app or live inside a chain.

---

### ğŸ§  DatabricksIQ, LakehouseIQ, & Assistant Features

- **DatabricksIQ**: LLM assistant for code, SQL, and UI help. Not exposed via API.
- **LakehouseIQ**: Natural language interface over structured data. Supports business-specific querying (e.g. â€œshow me sales by channel for Q2â€).
- **SQL AI Functions**:
  - Use `ai_query()` in dashboards or notebooks for lightweight inference.
  - Ideal for summarization, classification, or content tagging in batch.

---

## ğŸ§  Additional GenAI Study Notes (Cleaned & Expanded)

---

### âœ… Prompt Design & Output Control

- **Neutralizing Tone**: Instruct the LLM to rephrase emotionally charged input into neutral, professional language.
  - Example: `"Rewrite this message in a neutral, factual tone."`

- **Summarization Task**: Condensing paragraphs into 1â€“2 sentences = summarization. Evaluate with ROUGE (coverage) or BLEU (precision).

- **Output Structuring**: Use clear format instructions (e.g., â€œRespond in JSON with fields X, Y, Zâ€) and few-shot examples for reliable formatting.

---

### ğŸ” LLM Workflows & Agents

- **Multi-step LLM Workflow**: Used when tasks require chaining multiple steps (e.g., retrieve â†’ reason â†’ act).
  - Use LangChain or Databricks Agent Framework to orchestrate.
  
- **ReAct Framework**: Combines reasoning (thoughts) with tool actions â€” ideal for agentic LLMs making decisions and using tools.

- **Agent Frameworks**: Let LLMs select from registered tools (e.g., API lookup, SQL query, RAG retriever). Enables dynamic, goal-directed workflows.

---

### ğŸ’² Cost Management & Serving Patterns

- **Pay-per-token**: Ideal for low-volume use cases â€” no idle costs, scales with usage, no provisioning needed.

- **Low-Cost RAG Setup**: Efficient stack = Prompt + Retriever + LLM (no fine-tuning or agent logic unless required).

---

### ğŸ“š Feature Store & Real-Time Context

- **Databricks Feature Serving**: Serves structured, per-query data (e.g., user balance, delivery time) in real-time.
  - Use when data can't be embedded in advance.
  - Complements unstructured RAG by grounding LLMs in current state.

---

### ğŸ§± Chunking & Semantic Context

- **Section Headers in Chunks**: Boost semantic clarity in RAG. Helps retriever/embedding models infer context (e.g., â€œLeave Policyâ€ vs â€œDress Codeâ€).

---

### ğŸ›¡ï¸ Guardrails Overview

| Type        | Purpose                                                        |
|-------------|----------------------------------------------------------------|
| Safety      | Block harmful, toxic, or offensive responses                   |
| Security    | Prevent exposure of PII or sensitive data                      |
| Compliance  | Enforce legal/policy limits (e.g., no medical advice)          |
| Contextual  | Align output with session history/user role                    |
| Evaluation  | Apply metrics or human review to assess output quality         |

> Use a combination of system prompts, input filtering, and post-response validators.

---

### ğŸ§° Tooling Overview

#### Orchestration & Reasoning
- `LangChain` â€“ Build chains, agents, tool use
- `ReAct` â€“ Reasoning + acting loop for tool-using agents

#### Evaluation & Monitoring
- `MLflow` â€“ Track experiments, register models, deploy with PyFunc
  - PyFunc lets you wrap preprocessing + postprocessing logic
- **Evaluation Metrics**:
  - `BLEU` â€“ Translation accuracy
  - `ROUGE` â€“ Summarization recall
  - `MMLU` â€“ General LLM benchmark (academic + reasoning)
  - `NDCG` â€“ Normalized Discounted Cumulative Gain, ranks relevance in retrieval
  - `LLMs-as-a-judge` â€“ LLMs evaluate outputs for quality/consistency

---

### ğŸ§  Databricks-Specific Features

- **LakehouseIQ** â€“ Natural language interface to structured data; understands metadata and lineage.
- **MosaicML** â€“ Databricks-hosted open models; scalable + customizable foundation models.
- **Unity Catalog Volume** â€“ Managed volume-based storage with access control + lineage (e.g., for models, training data).
- **Inference Tables** â€“ Auto-log requests/responses from model serving endpoints â€” enables live debugging and monitoring.

---

### ğŸ§ª Embedding & Model Ecosystem

#### Key Python Libraries
- `unstructured` â€“ Extract text from PDFs, DOCX, HTML
- `pytesseract` â€“ OCR for image-to-text
- `langchain` â€“ Chain components, agents, tool use
- `mlflow` â€“ Model tracking, serving, versioning
- `sentence-transformers` â€“ Generate embeddings for retrieval
- `transformers` â€“ Load/tokenize Hugging Face models
- `pandas` â€“ Manipulate structured data
- `openai` â€“ Interface with OpenAI models (e.g., GPT-4)
- `faiss` â€“ Self-hosted vector search engine
- `scrapy` â€“ Web scraping (âš ï¸ Not used for RAG text extraction)
- `PyMuPDF`, `pdfplumber`, `doctr`, `Hugging Faceâ€™s datasets` â€“ Other extraction options

---

### ğŸ§  Databricks LLMs â€“ Simplified Reference

| Model Name         | Notes                                                                 |
|--------------------|-----------------------------------------------------------------------|
| **DBRX**           | Databricks' flagship LLM; strong at summarization and general reasoning |
| **MPT (7B / 30B)** | MosaicML models; efficient for chat/instruction-tuned tasks           |
| **LLaMA (2, 3.1, 3.3, 4)** | Metaâ€™s open models; high performance for reasoning & chat   |
| **CodeLLaMA**      | Optimized for code generation and understanding                       |
| **Claude (3.7, 4, Opus)** | Anthropic models; excellent for deep reasoning, summarization |
| **Whisper-large-v3** | OpenAIâ€™s speech-to-text model; used for audio transcription        |
| **BGE / GTE**      | Embedding models for RAG; fast and effective in dense retrieval       |
| **Dolly (1.0 / 2.0)** | Early Databricks models; not production-ready but good for learning |
| **DistilBERT**     | Lightweight transformer for classification or embedding               |

---

