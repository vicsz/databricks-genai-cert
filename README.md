# Databricks Certified Generative AI Engineer Associate â€“ Study Notes

## ðŸ§± Section 1: Design Applications
*How to architect GenAI solutions from use-case to execution flow*

---

### 1ï¸âƒ£ Prompt Design Rules

- **For structured output** â†’ Use few-shot prompting with clearly labeled examples.  
  Example:  
  `"Respond in JSON with keys â€˜summaryâ€™, â€˜sentimentâ€™, and â€˜entitiesâ€™."`

- **For classification tasks** â†’ Use zero-shot prompts with explicit classes:  
  `"Classify as one of: positive, neutral, negative."`

- **To elicit formatted output** â†’ Include formatting instructions **after** core context.  
  Models prioritize recent tokens â†’ put format instructions last.

- **Use system prompts** to establish tone/persona:  
  `"You are a financial assistant who responds concisely in bullet points."`

---

### 2ï¸âƒ£ Task-to-Model Mapping

| Business Need                          | Model Task                  | Preferred Approach                        |
|---------------------------------------|-----------------------------|-------------------------------------------|
| Text summarization                    | Summarization               | Prompt LLM with extractive/abstractive instructions |
| Structured data extraction (PDF/HTML) | Information Extraction      | Retrieval + field-specific prompt         |
| Classification                        | Zero-/Few-shot Classification | Prompt with example-labeled outputs     |
| Q&A over documents                    | Retrieval-Augmented Generation (RAG) | Embed + retrieve + prompt       |
| Multi-step reasoning or action        | Agent                        | Tool selection + LLM + memory             |

> ðŸ”‘ **Default**: Use **retrieval + prompting** before considering fine-tuning.

---

### 3ï¸âƒ£ Choosing Chain Components

**Input-based decisions:**
- Raw text â†’ Embed + index in Vector DB
- PDF/HTML â†’ Chunk + tag with metadata
- SQL or structured input â†’ Function calling

**Output-based decisions:**
- Summary â†’ Summarization chain
- Grounded answer â†’ RAG (Retriever + Generator)
- Action (e.g. booking) â†’ Agent with tools

---

### 4ï¸âƒ£ Translating Business Goals into Pipeline Design

Use this mental checklist:

- âœ… What is the **goal**?  
- âœ… What **input format** is expected?  
- âœ… What **output format** is required?  
- âœ… Is **factual accuracy** required?

**Example:**

> *"Generate ticket replies based on internal KB."*

- Input: ticket text  
- Output: concise email response  
- Pipeline:  
  - Chunk & embed KB  
  - Vector search relevant chunks  
  - Prompt LLM with user ticket + top docs  
  - Format output in helpdesk response style

---

### 5ï¸âƒ£ Multi-Stage Reasoning and Tool Use

- **Tools** = APIs or functions (e.g. calculator, search, lookup)
- **Agents** = LLMs that plan tool use across steps
- **Chain-of-thought prompting** â†’ Break complex reasoning into steps

**Example Tool Chain:**

1. Extract keywords from user question  
2. Use keywords to search KB  
3. Summarize retrieved docs  
4. Generate structured answer or invoke function

> ðŸ” RAG = static retrieval â†’ generate.  
> ðŸ§  Agents = dynamic reasoning â†’ tool â†’ loop.

---

### ðŸŽ¯ Key Rules of Thumb

- Use RAG before fine-tuning â†’ cheaper, safer, faster.
- Use system prompts to control tone and intent.
- Favor few-shot examples for structured output.
- Chain components based on **input/output**, not tool familiarity.
- Multi-stage reasoning? Consider **agents + tools**.

---
